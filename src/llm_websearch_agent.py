# -*- coding: utf-8 -*-
"""LLM_WebSearch_Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ScLmIobybko7ajt_DjgEIrrzihc-w5E2
"""

import requests
import pandas as pd
from bs4 import BeautifulSoup
from transformers import pipeline
from typing import List, Dict
from datetime import datetime

class GoogleSearch:
    def __init__(self, query: str, api_key: str, cse_id: str, num_results: int = 8):
        self.query = query
        self.api_key = api_key
        self.cse_id = cse_id
        self.num_results = num_results
        self.search_results = self.get_search_results()

    def get_search_results(self) -> list[dict]:
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": self.api_key,
            "cx": self.cse_id,
            "q": self.query,
            "num": self.num_results,
        }

        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            items = response.json().get("items", [])
            search_results = [
                {
                    "title": item.get("title"),
                    "link": item.get("link"),
                    "snippet": item.get("snippet"),
                }
                for item in items
            ]
            return search_results
        except requests.exceptions.RequestException as e:
            print(f"An error occurred: {e}")
            return []

class WebScraper:
    @staticmethod
    def scrape_content(url: str) -> str:
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")
            return soup.get_text(strip=True)
        except Exception as e:
            print(f"Error scraping {url}: {e}")
            return ""

class LLMQuestionAnsweringAgent:
    def __init__(self, search: GoogleSearch):
        self.search = search
        # We Use a pre-trained question-answering model (RoBERTa in this case)
        self.qa_model = pipeline("question-answering", model="deepset/roberta-base-squad2", device=0) #when gpu available device = 0

    def extract_info_from_links(self, prompt: str, ct: str, yr: int) -> List[Dict[str, str]]:
        responses = []

        for result in self.search.search_results:
            url = result['link']
            print(f"Scraping URL: {url}")

            content = WebScraper.scrape_content(url)
            if not content:
                continue

            # Use the model to extract homicide data from the content
            response = self.qa_model(question=prompt, context=content)
            answer = response.get('answer')
            score = response.get('score')

            # Collect answers with the title and link
            responses.append({
                "city": ct,
                "link": url,
                "homicide_data": answer,
                #"year": self.extract_year_from_prompt(prompt),
                "year": yr,
                "confidence": score
            })

        return responses

    # def extract_year_from_prompt(self, prompt: str) -> str:
    #     # Extract the year from the prompt (for year-based queries)
    #     years = [str(year) for year in range(2019, 2024)]  # 2019-2023 as default year range
    #     for year in years:
    #         if year in prompt:
    #             return year
    #     return "Unknown"

    def display_extracted_info(self, extracted_info):
        for i, info in enumerate(extracted_info, start=1):
            print(f"Result {i}:")
            print(f"City: {info['city']}")
            print(f"Link: {info['link']}")
            print(f"Homicide Data: {info['homicide_data']}")
            print(f"Year: {info['year']}")
            print(f"Confidence: {info['confidence']}\n")

    def create_table_from_results(self, extracted_info):
        # Create a structured table (e.g., DataFrame) from the extracted information
        table_data = []

        for info in extracted_info:
            data = {
                "City": info['city'],
                "Year": info['year'],
                "Homicide Data": info['homicide_data'],
                "Link": info['link'],
                "Confidence": info['confidence']
            }
            table_data.append(data)

        df = pd.DataFrame(table_data)
        return df

"""2019,2020,2021,2022,2023 New York,New Orleans,Los Angeles"""

cities = input("Enter cities (comma-separated): ").split(",")
years = input("Enter years (comma-separated): ").split(",")

cities = [city.strip() for city in cities]
years = [year.strip() for year in years]

# Initialize an empty list to store the data
all_extracted_info = []

# Loop over each city and each year
for city in cities:
    for year in years:
        # Construct a dynamic query based on user input
        query = f"Number of murders or homicides in the city of {city} in the year {year}"
        print(f"Searching for {query}")

        # Initialize the GoogleSearch and LLM agent for each city and year
        api_key = "AIzaSyDYheTN4nYPP61GN3VmMbzIcmoU3TqHvwk"
        cse_id = "601a4a054f8b145ad"
        google_search = GoogleSearch(query=query, api_key=api_key, cse_id=cse_id)
        llm_agent = LLMQuestionAnsweringAgent(search=google_search)

        # Extract homicide data for the current city and year
        prompt = f"What is the exact number of murders or homicides in {city} in the year {year}?"
        extracted_info = llm_agent.extract_info_from_links(prompt, city, year)

        # Add the extracted info to the list
        all_extracted_info.extend(extracted_info)

# Create the table with extracted data
full_df = llm_agent.create_table_from_results(all_extracted_info)

#old_df = full_df

# Group by 'City' and 'Year' and get the index of the row with maximum 'Confidence'
highest_confidence_indices = full_df.groupby(['City', 'Year'])['Confidence'].idxmax()

highest_confidence_df = full_df.loc[highest_confidence_indices]

highest_confidence_df